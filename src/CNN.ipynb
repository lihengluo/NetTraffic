{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-23T14:36:03.797984700Z",
     "start_time": "2023-08-23T14:36:03.637506900Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 读取数据， 第一行是列名\n",
    "labels = pd.read_csv('../data/labels.csv', header=0)\n",
    "featuress = pd.read_csv('../data/features.csv', header=0)\n",
    "# 将数据转换为numpy数组\n",
    "# 如果label为yes，转换为1，否则转换为0\n",
    "for i in range(len(labels)):\n",
    "    if labels['yes/no'][i] == 'yes':\n",
    "        labels['yes/no'][i] = 1\n",
    "    else:\n",
    "        labels['yes/no'][i] = 0\n",
    "labels = np.array(labels).astype(int)\n",
    "featuress = np.array(featuress).astype(float)\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(featuress, labels, test_size=0.3, random_state=0)\n",
    "\n",
    "# 将X_train 增加一个维度，值为1\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T14:36:06.885060200Z",
     "start_time": "2023-08-23T14:36:03.648837700Z"
    }
   },
   "id": "d396ee1a0475be3d"
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/495], Loss: 7.7850\n",
      "Epoch [1/10], Step [20/495], Loss: 6.5252\n",
      "Epoch [1/10], Step [30/495], Loss: 2.6516\n",
      "Epoch [1/10], Step [40/495], Loss: 2.0529\n",
      "Epoch [1/10], Step [50/495], Loss: 0.9378\n",
      "Epoch [1/10], Step [60/495], Loss: 68.2078\n",
      "Epoch [1/10], Step [70/495], Loss: 8.5421\n",
      "Epoch [1/10], Step [80/495], Loss: 9.4222\n",
      "Epoch [1/10], Step [90/495], Loss: 1.3931\n",
      "Epoch [1/10], Step [100/495], Loss: 17.7477\n",
      "Epoch [1/10], Step [110/495], Loss: 1.9715\n",
      "Epoch [1/10], Step [120/495], Loss: 1.4531\n",
      "Epoch [1/10], Step [130/495], Loss: 1.3506\n",
      "Epoch [1/10], Step [140/495], Loss: 2.4509\n",
      "Epoch [1/10], Step [150/495], Loss: 1.7104\n",
      "Epoch [1/10], Step [160/495], Loss: 4.6592\n",
      "Epoch [1/10], Step [170/495], Loss: 13.2153\n",
      "Epoch [1/10], Step [180/495], Loss: 0.9928\n",
      "Epoch [1/10], Step [190/495], Loss: 1.1460\n",
      "Epoch [1/10], Step [200/495], Loss: 131.2766\n",
      "Epoch [1/10], Step [210/495], Loss: 25.7539\n",
      "Epoch [1/10], Step [220/495], Loss: 88.3010\n",
      "Epoch [1/10], Step [230/495], Loss: 17.3449\n",
      "Epoch [1/10], Step [240/495], Loss: 43.6076\n",
      "Epoch [1/10], Step [250/495], Loss: 37.8050\n",
      "Epoch [1/10], Step [260/495], Loss: 10.3099\n",
      "Epoch [1/10], Step [270/495], Loss: 9.9822\n",
      "Epoch [1/10], Step [280/495], Loss: 3.0556\n",
      "Epoch [1/10], Step [290/495], Loss: 1031.7340\n",
      "Epoch [1/10], Step [300/495], Loss: 20.1005\n",
      "Epoch [1/10], Step [310/495], Loss: 15.6516\n",
      "Epoch [1/10], Step [320/495], Loss: 161.4916\n",
      "Epoch [1/10], Step [330/495], Loss: 33.6794\n",
      "Epoch [1/10], Step [340/495], Loss: 27.9224\n",
      "Epoch [1/10], Step [350/495], Loss: 29.2652\n",
      "Epoch [1/10], Step [360/495], Loss: 516.4632\n",
      "Epoch [1/10], Step [370/495], Loss: 18.8363\n",
      "Epoch [1/10], Step [380/495], Loss: 45.5455\n",
      "Epoch [1/10], Step [390/495], Loss: 50.0468\n",
      "Epoch [1/10], Step [400/495], Loss: 34.2815\n",
      "Epoch [1/10], Step [410/495], Loss: 11.9129\n",
      "Epoch [1/10], Step [420/495], Loss: 9.3347\n",
      "Epoch [1/10], Step [430/495], Loss: 12.8112\n",
      "Epoch [1/10], Step [440/495], Loss: 7.0620\n",
      "Epoch [1/10], Step [450/495], Loss: 6.6727\n",
      "Epoch [1/10], Step [460/495], Loss: 3.0424\n",
      "Epoch [1/10], Step [470/495], Loss: 11.5247\n",
      "Epoch [1/10], Step [480/495], Loss: 1.1686\n",
      "Epoch [1/10], Step [490/495], Loss: 2.8765\n",
      "Epoch [2/10], Step [10/495], Loss: 1.0704\n",
      "Epoch [2/10], Step [20/495], Loss: 1.3828\n",
      "Epoch [2/10], Step [30/495], Loss: 1.2747\n",
      "Epoch [2/10], Step [40/495], Loss: 2.4986\n",
      "Epoch [2/10], Step [50/495], Loss: 2.9132\n",
      "Epoch [2/10], Step [60/495], Loss: 0.7572\n",
      "Epoch [2/10], Step [70/495], Loss: 0.8098\n",
      "Epoch [2/10], Step [80/495], Loss: 0.6298\n",
      "Epoch [2/10], Step [90/495], Loss: 1.9179\n",
      "Epoch [2/10], Step [100/495], Loss: 62.6564\n",
      "Epoch [2/10], Step [110/495], Loss: 1.6880\n",
      "Epoch [2/10], Step [120/495], Loss: 2.9649\n",
      "Epoch [2/10], Step [130/495], Loss: 5.2206\n",
      "Epoch [2/10], Step [140/495], Loss: 6.5830\n",
      "Epoch [2/10], Step [150/495], Loss: 0.4318\n",
      "Epoch [2/10], Step [160/495], Loss: 3.0115\n",
      "Epoch [2/10], Step [170/495], Loss: 1.4388\n",
      "Epoch [2/10], Step [180/495], Loss: 2.3500\n",
      "Epoch [2/10], Step [190/495], Loss: 1.1591\n",
      "Epoch [2/10], Step [200/495], Loss: 9.3344\n",
      "Epoch [2/10], Step [210/495], Loss: 1.5609\n",
      "Epoch [2/10], Step [220/495], Loss: 2.0465\n",
      "Epoch [2/10], Step [230/495], Loss: 1.1175\n",
      "Epoch [2/10], Step [240/495], Loss: 1.6262\n",
      "Epoch [2/10], Step [250/495], Loss: 1.0242\n",
      "Epoch [2/10], Step [260/495], Loss: 9.6268\n",
      "Epoch [2/10], Step [270/495], Loss: 1.5479\n",
      "Epoch [2/10], Step [280/495], Loss: 0.5707\n",
      "Epoch [2/10], Step [290/495], Loss: 4.8296\n",
      "Epoch [2/10], Step [300/495], Loss: 0.7445\n",
      "Epoch [2/10], Step [310/495], Loss: 0.8313\n",
      "Epoch [2/10], Step [320/495], Loss: 0.5672\n",
      "Epoch [2/10], Step [330/495], Loss: 0.9076\n",
      "Epoch [2/10], Step [340/495], Loss: 1.0634\n",
      "Epoch [2/10], Step [350/495], Loss: 1.2037\n",
      "Epoch [2/10], Step [360/495], Loss: 2.0343\n",
      "Epoch [2/10], Step [370/495], Loss: 1.9605\n",
      "Epoch [2/10], Step [380/495], Loss: 0.9254\n",
      "Epoch [2/10], Step [390/495], Loss: 0.9828\n",
      "Epoch [2/10], Step [400/495], Loss: 1.0924\n",
      "Epoch [2/10], Step [410/495], Loss: 0.6698\n",
      "Epoch [2/10], Step [420/495], Loss: 0.9090\n",
      "Epoch [2/10], Step [430/495], Loss: 0.8875\n",
      "Epoch [2/10], Step [440/495], Loss: 1.3481\n",
      "Epoch [2/10], Step [450/495], Loss: 25.1780\n",
      "Epoch [2/10], Step [460/495], Loss: 42.3057\n",
      "Epoch [2/10], Step [470/495], Loss: 21.2996\n",
      "Epoch [2/10], Step [480/495], Loss: 170.7486\n",
      "Epoch [2/10], Step [490/495], Loss: 36.7373\n",
      "Epoch [3/10], Step [10/495], Loss: 20.9020\n",
      "Epoch [3/10], Step [20/495], Loss: 3.3375\n",
      "Epoch [3/10], Step [30/495], Loss: 4.8073\n",
      "Epoch [3/10], Step [40/495], Loss: 8.5634\n",
      "Epoch [3/10], Step [50/495], Loss: 7.9679\n",
      "Epoch [3/10], Step [60/495], Loss: 3.1856\n",
      "Epoch [3/10], Step [70/495], Loss: 1.9314\n",
      "Epoch [3/10], Step [80/495], Loss: 1.2466\n",
      "Epoch [3/10], Step [90/495], Loss: 1.1952\n",
      "Epoch [3/10], Step [100/495], Loss: 3.4666\n",
      "Epoch [3/10], Step [110/495], Loss: 0.8369\n",
      "Epoch [3/10], Step [120/495], Loss: 8.8119\n",
      "Epoch [3/10], Step [130/495], Loss: 1.0334\n",
      "Epoch [3/10], Step [140/495], Loss: 2.2046\n",
      "Epoch [3/10], Step [150/495], Loss: 1.9117\n",
      "Epoch [3/10], Step [160/495], Loss: 1.3849\n",
      "Epoch [3/10], Step [170/495], Loss: 2.1066\n",
      "Epoch [3/10], Step [180/495], Loss: 1.5371\n",
      "Epoch [3/10], Step [190/495], Loss: 12.5907\n",
      "Epoch [3/10], Step [200/495], Loss: 23.2862\n",
      "Epoch [3/10], Step [210/495], Loss: 17.5073\n",
      "Epoch [3/10], Step [220/495], Loss: 36.9852\n",
      "Epoch [3/10], Step [230/495], Loss: 30.7089\n",
      "Epoch [3/10], Step [240/495], Loss: 17.6721\n",
      "Epoch [3/10], Step [250/495], Loss: 98.2828\n",
      "Epoch [3/10], Step [260/495], Loss: 33.8382\n",
      "Epoch [3/10], Step [270/495], Loss: 3.1043\n",
      "Epoch [3/10], Step [280/495], Loss: 6.1902\n",
      "Epoch [3/10], Step [290/495], Loss: 3.9302\n",
      "Epoch [3/10], Step [300/495], Loss: 4.1735\n",
      "Epoch [3/10], Step [310/495], Loss: 3.3044\n",
      "Epoch [3/10], Step [320/495], Loss: 2.5316\n",
      "Epoch [3/10], Step [330/495], Loss: 30.9446\n",
      "Epoch [3/10], Step [340/495], Loss: 1.2401\n",
      "Epoch [3/10], Step [350/495], Loss: 4.2390\n",
      "Epoch [3/10], Step [360/495], Loss: 5.4590\n",
      "Epoch [3/10], Step [370/495], Loss: 0.7768\n",
      "Epoch [3/10], Step [380/495], Loss: 1.4457\n",
      "Epoch [3/10], Step [390/495], Loss: 1.4484\n",
      "Epoch [3/10], Step [400/495], Loss: 1.4862\n",
      "Epoch [3/10], Step [410/495], Loss: 1.4357\n",
      "Epoch [3/10], Step [420/495], Loss: 1.9572\n",
      "Epoch [3/10], Step [430/495], Loss: 1.3356\n",
      "Epoch [3/10], Step [440/495], Loss: 1.1017\n",
      "Epoch [3/10], Step [450/495], Loss: 6.2955\n",
      "Epoch [3/10], Step [460/495], Loss: 1.7628\n",
      "Epoch [3/10], Step [470/495], Loss: 1.0384\n",
      "Epoch [3/10], Step [480/495], Loss: 3.9257\n",
      "Epoch [3/10], Step [490/495], Loss: 1.1224\n",
      "Epoch [4/10], Step [10/495], Loss: 2.1596\n",
      "Epoch [4/10], Step [20/495], Loss: 13.6409\n",
      "Epoch [4/10], Step [30/495], Loss: 1.6366\n",
      "Epoch [4/10], Step [40/495], Loss: 0.7421\n",
      "Epoch [4/10], Step [50/495], Loss: 0.7823\n",
      "Epoch [4/10], Step [60/495], Loss: 0.6863\n",
      "Epoch [4/10], Step [70/495], Loss: 1.1837\n",
      "Epoch [4/10], Step [80/495], Loss: 0.8993\n",
      "Epoch [4/10], Step [90/495], Loss: 1.7209\n",
      "Epoch [4/10], Step [100/495], Loss: 12.8988\n",
      "Epoch [4/10], Step [110/495], Loss: 1.6318\n",
      "Epoch [4/10], Step [120/495], Loss: 2.1889\n",
      "Epoch [4/10], Step [130/495], Loss: 1.6632\n",
      "Epoch [4/10], Step [140/495], Loss: 1.4408\n",
      "Epoch [4/10], Step [150/495], Loss: 13.0409\n",
      "Epoch [4/10], Step [160/495], Loss: 1.6453\n",
      "Epoch [4/10], Step [170/495], Loss: 53.1950\n",
      "Epoch [4/10], Step [180/495], Loss: 9.0992\n",
      "Epoch [4/10], Step [190/495], Loss: 3.6004\n",
      "Epoch [4/10], Step [200/495], Loss: 2.7714\n",
      "Epoch [4/10], Step [210/495], Loss: 3.6496\n",
      "Epoch [4/10], Step [220/495], Loss: 0.6226\n",
      "Epoch [4/10], Step [230/495], Loss: 7.4814\n",
      "Epoch [4/10], Step [240/495], Loss: 1.3657\n",
      "Epoch [4/10], Step [250/495], Loss: 10.9913\n",
      "Epoch [4/10], Step [260/495], Loss: 5.6210\n",
      "Epoch [4/10], Step [270/495], Loss: 1.4294\n",
      "Epoch [4/10], Step [280/495], Loss: 1.0498\n",
      "Epoch [4/10], Step [290/495], Loss: 0.4749\n",
      "Epoch [4/10], Step [300/495], Loss: 0.6966\n",
      "Epoch [4/10], Step [310/495], Loss: 2.8375\n",
      "Epoch [4/10], Step [320/495], Loss: 10.0238\n",
      "Epoch [4/10], Step [330/495], Loss: 6.6896\n",
      "Epoch [4/10], Step [340/495], Loss: 14.3326\n",
      "Epoch [4/10], Step [350/495], Loss: 53.4914\n",
      "Epoch [4/10], Step [360/495], Loss: 4.7718\n",
      "Epoch [4/10], Step [370/495], Loss: 1.3497\n",
      "Epoch [4/10], Step [380/495], Loss: 3.4923\n",
      "Epoch [4/10], Step [390/495], Loss: 5.0890\n",
      "Epoch [4/10], Step [400/495], Loss: 203.2892\n",
      "Epoch [4/10], Step [410/495], Loss: 17.9345\n",
      "Epoch [4/10], Step [420/495], Loss: 1.4512\n",
      "Epoch [4/10], Step [430/495], Loss: 1.1804\n",
      "Epoch [4/10], Step [440/495], Loss: 1.3280\n",
      "Epoch [4/10], Step [450/495], Loss: 0.8806\n",
      "Epoch [4/10], Step [460/495], Loss: 4.8475\n",
      "Epoch [4/10], Step [470/495], Loss: 1.0219\n",
      "Epoch [4/10], Step [480/495], Loss: 0.8319\n",
      "Epoch [4/10], Step [490/495], Loss: 2.1566\n",
      "Epoch [5/10], Step [10/495], Loss: 1.4588\n",
      "Epoch [5/10], Step [20/495], Loss: 1.1655\n",
      "Epoch [5/10], Step [30/495], Loss: 5.6821\n",
      "Epoch [5/10], Step [40/495], Loss: 11.7323\n",
      "Epoch [5/10], Step [50/495], Loss: 1.7901\n",
      "Epoch [5/10], Step [60/495], Loss: 0.6078\n",
      "Epoch [5/10], Step [70/495], Loss: 8.1371\n",
      "Epoch [5/10], Step [80/495], Loss: 5.0711\n",
      "Epoch [5/10], Step [90/495], Loss: 4.2868\n",
      "Epoch [5/10], Step [100/495], Loss: 2.0104\n",
      "Epoch [5/10], Step [110/495], Loss: 1.2374\n",
      "Epoch [5/10], Step [120/495], Loss: 1.1008\n",
      "Epoch [5/10], Step [130/495], Loss: 6.6509\n",
      "Epoch [5/10], Step [140/495], Loss: 21.0172\n",
      "Epoch [5/10], Step [150/495], Loss: 20.5755\n",
      "Epoch [5/10], Step [160/495], Loss: 48.9523\n",
      "Epoch [5/10], Step [170/495], Loss: 610.2849\n",
      "Epoch [5/10], Step [180/495], Loss: 52.2873\n",
      "Epoch [5/10], Step [190/495], Loss: 31.1001\n",
      "Epoch [5/10], Step [200/495], Loss: 19.2314\n",
      "Epoch [5/10], Step [210/495], Loss: 5.2314\n",
      "Epoch [5/10], Step [220/495], Loss: 20.3274\n",
      "Epoch [5/10], Step [230/495], Loss: 2.9435\n",
      "Epoch [5/10], Step [240/495], Loss: 2.1983\n",
      "Epoch [5/10], Step [250/495], Loss: 125.1362\n",
      "Epoch [5/10], Step [260/495], Loss: 5.0601\n",
      "Epoch [5/10], Step [270/495], Loss: 22.6214\n",
      "Epoch [5/10], Step [280/495], Loss: 0.8324\n",
      "Epoch [5/10], Step [290/495], Loss: 16.3440\n",
      "Epoch [5/10], Step [300/495], Loss: 2.6350\n",
      "Epoch [5/10], Step [310/495], Loss: 3.0049\n",
      "Epoch [5/10], Step [320/495], Loss: 3.5251\n",
      "Epoch [5/10], Step [330/495], Loss: 2.8289\n",
      "Epoch [5/10], Step [340/495], Loss: 1.7275\n",
      "Epoch [5/10], Step [350/495], Loss: 1.3718\n",
      "Epoch [5/10], Step [360/495], Loss: 1.5758\n",
      "Epoch [5/10], Step [370/495], Loss: 1.9040\n",
      "Epoch [5/10], Step [380/495], Loss: 1.5186\n",
      "Epoch [5/10], Step [390/495], Loss: 1.1383\n",
      "Epoch [5/10], Step [400/495], Loss: 1.0828\n",
      "Epoch [5/10], Step [410/495], Loss: 4.7089\n",
      "Epoch [5/10], Step [420/495], Loss: 0.9109\n",
      "Epoch [5/10], Step [430/495], Loss: 1.4713\n",
      "Epoch [5/10], Step [440/495], Loss: 16.3705\n",
      "Epoch [5/10], Step [450/495], Loss: 0.8224\n",
      "Epoch [5/10], Step [460/495], Loss: 10.1484\n",
      "Epoch [5/10], Step [470/495], Loss: 4.3165\n",
      "Epoch [5/10], Step [480/495], Loss: 13.5052\n",
      "Epoch [5/10], Step [490/495], Loss: 1.3659\n",
      "Epoch [6/10], Step [10/495], Loss: 8.2715\n",
      "Epoch [6/10], Step [20/495], Loss: 4.5439\n",
      "Epoch [6/10], Step [30/495], Loss: 3.6765\n",
      "Epoch [6/10], Step [40/495], Loss: 4.3203\n",
      "Epoch [6/10], Step [50/495], Loss: 3.6471\n",
      "Epoch [6/10], Step [60/495], Loss: 74.7409\n",
      "Epoch [6/10], Step [70/495], Loss: 1.2106\n",
      "Epoch [6/10], Step [80/495], Loss: 0.8731\n",
      "Epoch [6/10], Step [90/495], Loss: 1.6323\n",
      "Epoch [6/10], Step [100/495], Loss: 1.2705\n",
      "Epoch [6/10], Step [110/495], Loss: 1.6987\n",
      "Epoch [6/10], Step [120/495], Loss: 0.5828\n",
      "Epoch [6/10], Step [130/495], Loss: 1.0200\n",
      "Epoch [6/10], Step [140/495], Loss: 103.3060\n",
      "Epoch [6/10], Step [150/495], Loss: 27.4004\n",
      "Epoch [6/10], Step [160/495], Loss: 3.0476\n",
      "Epoch [6/10], Step [170/495], Loss: 2.6779\n",
      "Epoch [6/10], Step [180/495], Loss: 0.9784\n",
      "Epoch [6/10], Step [190/495], Loss: 0.6666\n",
      "Epoch [6/10], Step [200/495], Loss: 10.5957\n",
      "Epoch [6/10], Step [210/495], Loss: 1.4335\n",
      "Epoch [6/10], Step [220/495], Loss: 0.6920\n",
      "Epoch [6/10], Step [230/495], Loss: 0.7161\n",
      "Epoch [6/10], Step [240/495], Loss: 22.6200\n",
      "Epoch [6/10], Step [250/495], Loss: 0.7098\n",
      "Epoch [6/10], Step [260/495], Loss: 11.1731\n",
      "Epoch [6/10], Step [270/495], Loss: 28.0085\n",
      "Epoch [6/10], Step [280/495], Loss: 20.0402\n",
      "Epoch [6/10], Step [290/495], Loss: 21.2387\n",
      "Epoch [6/10], Step [300/495], Loss: 55.2821\n",
      "Epoch [6/10], Step [310/495], Loss: 19.0954\n",
      "Epoch [6/10], Step [320/495], Loss: 9.4041\n",
      "Epoch [6/10], Step [330/495], Loss: 28.2856\n",
      "Epoch [6/10], Step [340/495], Loss: 22.4131\n",
      "Epoch [6/10], Step [350/495], Loss: 15.6656\n",
      "Epoch [6/10], Step [360/495], Loss: 2.6277\n",
      "Epoch [6/10], Step [370/495], Loss: 7.7876\n",
      "Epoch [6/10], Step [380/495], Loss: 6.6238\n",
      "Epoch [6/10], Step [390/495], Loss: 8.4222\n",
      "Epoch [6/10], Step [400/495], Loss: 6.9344\n",
      "Epoch [6/10], Step [410/495], Loss: 29.5347\n",
      "Epoch [6/10], Step [420/495], Loss: 9.2297\n",
      "Epoch [6/10], Step [430/495], Loss: 12.7589\n",
      "Epoch [6/10], Step [440/495], Loss: 44.0992\n",
      "Epoch [6/10], Step [450/495], Loss: 0.9445\n",
      "Epoch [6/10], Step [460/495], Loss: 7.8237\n",
      "Epoch [6/10], Step [470/495], Loss: 2.6649\n",
      "Epoch [6/10], Step [480/495], Loss: 3.5821\n",
      "Epoch [6/10], Step [490/495], Loss: 2.9869\n",
      "Epoch [7/10], Step [10/495], Loss: 5.1242\n",
      "Epoch [7/10], Step [20/495], Loss: 32.5407\n",
      "Epoch [7/10], Step [30/495], Loss: 0.9526\n",
      "Epoch [7/10], Step [40/495], Loss: 26.5085\n",
      "Epoch [7/10], Step [50/495], Loss: 2.4749\n",
      "Epoch [7/10], Step [60/495], Loss: 10.4460\n",
      "Epoch [7/10], Step [70/495], Loss: 40.2752\n",
      "Epoch [7/10], Step [80/495], Loss: 70.7455\n",
      "Epoch [7/10], Step [90/495], Loss: 503.9803\n",
      "Epoch [7/10], Step [100/495], Loss: 25.9499\n",
      "Epoch [7/10], Step [110/495], Loss: 27.1646\n",
      "Epoch [7/10], Step [120/495], Loss: 40.6649\n",
      "Epoch [7/10], Step [130/495], Loss: 66.6193\n",
      "Epoch [7/10], Step [140/495], Loss: 16.6865\n",
      "Epoch [7/10], Step [150/495], Loss: 9.8379\n",
      "Epoch [7/10], Step [160/495], Loss: 4.1568\n",
      "Epoch [7/10], Step [170/495], Loss: 56.3705\n",
      "Epoch [7/10], Step [180/495], Loss: 7.0982\n",
      "Epoch [7/10], Step [190/495], Loss: 5.0786\n",
      "Epoch [7/10], Step [200/495], Loss: 4.7434\n",
      "Epoch [7/10], Step [210/495], Loss: 6.4844\n",
      "Epoch [7/10], Step [220/495], Loss: 8.4244\n",
      "Epoch [7/10], Step [230/495], Loss: 4.1786\n",
      "Epoch [7/10], Step [240/495], Loss: 0.7184\n",
      "Epoch [7/10], Step [250/495], Loss: 1.2000\n",
      "Epoch [7/10], Step [260/495], Loss: 11.4978\n",
      "Epoch [7/10], Step [270/495], Loss: 1.0127\n",
      "Epoch [7/10], Step [280/495], Loss: 1.3846\n",
      "Epoch [7/10], Step [290/495], Loss: 20.5119\n",
      "Epoch [7/10], Step [300/495], Loss: 1.0919\n",
      "Epoch [7/10], Step [310/495], Loss: 1.1256\n",
      "Epoch [7/10], Step [320/495], Loss: 64.0070\n",
      "Epoch [7/10], Step [330/495], Loss: 0.9134\n",
      "Epoch [7/10], Step [340/495], Loss: 12.8956\n",
      "Epoch [7/10], Step [350/495], Loss: 115.4899\n",
      "Epoch [7/10], Step [360/495], Loss: 23.3209\n",
      "Epoch [7/10], Step [370/495], Loss: 6.5416\n",
      "Epoch [7/10], Step [380/495], Loss: 10.5364\n",
      "Epoch [7/10], Step [390/495], Loss: 11.2720\n",
      "Epoch [7/10], Step [400/495], Loss: 60.9612\n",
      "Epoch [7/10], Step [410/495], Loss: 2.2505\n",
      "Epoch [7/10], Step [420/495], Loss: 2.9866\n",
      "Epoch [7/10], Step [430/495], Loss: 8.4706\n",
      "Epoch [7/10], Step [440/495], Loss: 1.7186\n",
      "Epoch [7/10], Step [450/495], Loss: 2.7136\n",
      "Epoch [7/10], Step [460/495], Loss: 7.1056\n",
      "Epoch [7/10], Step [470/495], Loss: 11.6301\n",
      "Epoch [7/10], Step [480/495], Loss: 1.8233\n",
      "Epoch [7/10], Step [490/495], Loss: 2.5165\n",
      "Epoch [8/10], Step [10/495], Loss: 1.1775\n",
      "Epoch [8/10], Step [20/495], Loss: 1.2451\n",
      "Epoch [8/10], Step [30/495], Loss: 0.8556\n",
      "Epoch [8/10], Step [40/495], Loss: 0.6165\n",
      "Epoch [8/10], Step [50/495], Loss: 0.8247\n",
      "Epoch [8/10], Step [60/495], Loss: 0.8238\n",
      "Epoch [8/10], Step [70/495], Loss: 2.6981\n",
      "Epoch [8/10], Step [80/495], Loss: 2.5077\n",
      "Epoch [8/10], Step [90/495], Loss: 15.5296\n",
      "Epoch [8/10], Step [100/495], Loss: 14.9290\n",
      "Epoch [8/10], Step [110/495], Loss: 2.0708\n",
      "Epoch [8/10], Step [120/495], Loss: 2.2786\n",
      "Epoch [8/10], Step [130/495], Loss: 7.6482\n",
      "Epoch [8/10], Step [140/495], Loss: 24.0403\n",
      "Epoch [8/10], Step [150/495], Loss: 49.0722\n",
      "Epoch [8/10], Step [160/495], Loss: 29.6482\n",
      "Epoch [8/10], Step [170/495], Loss: 8.9281\n",
      "Epoch [8/10], Step [180/495], Loss: 32.8013\n",
      "Epoch [8/10], Step [190/495], Loss: 17.8013\n",
      "Epoch [8/10], Step [200/495], Loss: 23.0719\n",
      "Epoch [8/10], Step [210/495], Loss: 12.9951\n",
      "Epoch [8/10], Step [220/495], Loss: 4.0618\n",
      "Epoch [8/10], Step [230/495], Loss: 2.2430\n",
      "Epoch [8/10], Step [240/495], Loss: 52.6751\n",
      "Epoch [8/10], Step [250/495], Loss: 11.0149\n",
      "Epoch [8/10], Step [260/495], Loss: 5.5259\n",
      "Epoch [8/10], Step [270/495], Loss: 6.3443\n",
      "Epoch [8/10], Step [280/495], Loss: 287.0558\n",
      "Epoch [8/10], Step [290/495], Loss: 3.4775\n",
      "Epoch [8/10], Step [300/495], Loss: 49.7898\n",
      "Epoch [8/10], Step [310/495], Loss: 3.0807\n",
      "Epoch [8/10], Step [320/495], Loss: 4.6458\n",
      "Epoch [8/10], Step [330/495], Loss: 1.2970\n",
      "Epoch [8/10], Step [340/495], Loss: 0.8317\n",
      "Epoch [8/10], Step [350/495], Loss: 12.8825\n",
      "Epoch [8/10], Step [360/495], Loss: 1.1197\n",
      "Epoch [8/10], Step [370/495], Loss: 5.3697\n",
      "Epoch [8/10], Step [380/495], Loss: 0.9915\n",
      "Epoch [8/10], Step [390/495], Loss: 1.2748\n",
      "Epoch [8/10], Step [400/495], Loss: 3.6682\n",
      "Epoch [8/10], Step [410/495], Loss: 0.9123\n",
      "Epoch [8/10], Step [420/495], Loss: 5.8745\n",
      "Epoch [8/10], Step [430/495], Loss: 1.4772\n",
      "Epoch [8/10], Step [440/495], Loss: 2.0212\n",
      "Epoch [8/10], Step [450/495], Loss: 5.1704\n",
      "Epoch [8/10], Step [460/495], Loss: 0.8876\n",
      "Epoch [8/10], Step [470/495], Loss: 9.1285\n",
      "Epoch [8/10], Step [480/495], Loss: 0.6159\n",
      "Epoch [8/10], Step [490/495], Loss: 1.5804\n",
      "Epoch [9/10], Step [10/495], Loss: 1.0724\n",
      "Epoch [9/10], Step [20/495], Loss: 1.3506\n",
      "Epoch [9/10], Step [30/495], Loss: 0.7437\n",
      "Epoch [9/10], Step [40/495], Loss: 6.1272\n",
      "Epoch [9/10], Step [50/495], Loss: 16.2970\n",
      "Epoch [9/10], Step [60/495], Loss: 10.5730\n",
      "Epoch [9/10], Step [70/495], Loss: 3.1140\n",
      "Epoch [9/10], Step [80/495], Loss: 1.6664\n",
      "Epoch [9/10], Step [90/495], Loss: 2.0694\n",
      "Epoch [9/10], Step [100/495], Loss: 4.8059\n",
      "Epoch [9/10], Step [110/495], Loss: 1.0811\n",
      "Epoch [9/10], Step [120/495], Loss: 0.7951\n",
      "Epoch [9/10], Step [130/495], Loss: 1.0244\n",
      "Epoch [9/10], Step [140/495], Loss: 0.9783\n",
      "Epoch [9/10], Step [150/495], Loss: 1.9388\n",
      "Epoch [9/10], Step [160/495], Loss: 0.7773\n",
      "Epoch [9/10], Step [170/495], Loss: 0.7825\n",
      "Epoch [9/10], Step [180/495], Loss: 0.8207\n",
      "Epoch [9/10], Step [190/495], Loss: 1.3692\n",
      "Epoch [9/10], Step [200/495], Loss: 3.7606\n",
      "Epoch [9/10], Step [210/495], Loss: 7.7548\n",
      "Epoch [9/10], Step [220/495], Loss: 0.7030\n",
      "Epoch [9/10], Step [230/495], Loss: 1.6099\n",
      "Epoch [9/10], Step [240/495], Loss: 0.5685\n",
      "Epoch [9/10], Step [250/495], Loss: 11.0512\n",
      "Epoch [9/10], Step [260/495], Loss: 6.6771\n",
      "Epoch [9/10], Step [270/495], Loss: 6.6621\n",
      "Epoch [9/10], Step [280/495], Loss: 80.5317\n",
      "Epoch [9/10], Step [290/495], Loss: 2.2866\n",
      "Epoch [9/10], Step [300/495], Loss: 2.4463\n",
      "Epoch [9/10], Step [310/495], Loss: 5.4723\n",
      "Epoch [9/10], Step [320/495], Loss: 8.2481\n",
      "Epoch [9/10], Step [330/495], Loss: 3.7722\n",
      "Epoch [9/10], Step [340/495], Loss: 24.7906\n",
      "Epoch [9/10], Step [350/495], Loss: 0.8560\n",
      "Epoch [9/10], Step [360/495], Loss: 39.4087\n",
      "Epoch [9/10], Step [370/495], Loss: 0.9085\n",
      "Epoch [9/10], Step [380/495], Loss: 0.8151\n",
      "Epoch [9/10], Step [390/495], Loss: 2.1556\n",
      "Epoch [9/10], Step [400/495], Loss: 22.3602\n",
      "Epoch [9/10], Step [410/495], Loss: 1.8585\n",
      "Epoch [9/10], Step [420/495], Loss: 1.8949\n",
      "Epoch [9/10], Step [430/495], Loss: 0.8469\n",
      "Epoch [9/10], Step [440/495], Loss: 0.4283\n",
      "Epoch [9/10], Step [450/495], Loss: 0.5432\n",
      "Epoch [9/10], Step [460/495], Loss: 1.3069\n",
      "Epoch [9/10], Step [470/495], Loss: 7.7597\n",
      "Epoch [9/10], Step [480/495], Loss: 0.5649\n",
      "Epoch [9/10], Step [490/495], Loss: 0.7104\n",
      "Epoch [10/10], Step [10/495], Loss: 1.1829\n",
      "Epoch [10/10], Step [20/495], Loss: 0.9514\n",
      "Epoch [10/10], Step [30/495], Loss: 1.0173\n",
      "Epoch [10/10], Step [40/495], Loss: 0.8313\n",
      "Epoch [10/10], Step [50/495], Loss: 9.9623\n",
      "Epoch [10/10], Step [60/495], Loss: 4.2841\n",
      "Epoch [10/10], Step [70/495], Loss: 1.7661\n",
      "Epoch [10/10], Step [80/495], Loss: 1.1437\n",
      "Epoch [10/10], Step [90/495], Loss: 3.7220\n",
      "Epoch [10/10], Step [100/495], Loss: 0.4571\n",
      "Epoch [10/10], Step [110/495], Loss: 0.4937\n",
      "Epoch [10/10], Step [120/495], Loss: 2.6463\n",
      "Epoch [10/10], Step [130/495], Loss: 0.5822\n",
      "Epoch [10/10], Step [140/495], Loss: 0.6903\n",
      "Epoch [10/10], Step [150/495], Loss: 2.7954\n",
      "Epoch [10/10], Step [160/495], Loss: 10.3802\n",
      "Epoch [10/10], Step [170/495], Loss: 0.9324\n",
      "Epoch [10/10], Step [180/495], Loss: 0.7152\n",
      "Epoch [10/10], Step [190/495], Loss: 0.4058\n",
      "Epoch [10/10], Step [200/495], Loss: 0.8373\n",
      "Epoch [10/10], Step [210/495], Loss: 0.4519\n",
      "Epoch [10/10], Step [220/495], Loss: 0.6545\n",
      "Epoch [10/10], Step [230/495], Loss: 0.6933\n",
      "Epoch [10/10], Step [240/495], Loss: 0.8006\n",
      "Epoch [10/10], Step [250/495], Loss: 3.4532\n",
      "Epoch [10/10], Step [260/495], Loss: 108.9271\n",
      "Epoch [10/10], Step [270/495], Loss: 550.8138\n",
      "Epoch [10/10], Step [280/495], Loss: 144.3246\n",
      "Epoch [10/10], Step [290/495], Loss: 9.0820\n",
      "Epoch [10/10], Step [300/495], Loss: 15.0707\n",
      "Epoch [10/10], Step [310/495], Loss: 346.1717\n",
      "Epoch [10/10], Step [320/495], Loss: 15.3433\n",
      "Epoch [10/10], Step [330/495], Loss: 3.7200\n",
      "Epoch [10/10], Step [340/495], Loss: 2.6385\n",
      "Epoch [10/10], Step [350/495], Loss: 2.9490\n",
      "Epoch [10/10], Step [360/495], Loss: 1.8871\n",
      "Epoch [10/10], Step [370/495], Loss: 1.8742\n",
      "Epoch [10/10], Step [380/495], Loss: 3.9023\n",
      "Epoch [10/10], Step [390/495], Loss: 1.2039\n",
      "Epoch [10/10], Step [400/495], Loss: 25.6463\n",
      "Epoch [10/10], Step [410/495], Loss: 0.8089\n",
      "Epoch [10/10], Step [420/495], Loss: 1.0691\n",
      "Epoch [10/10], Step [430/495], Loss: 1.8559\n",
      "Epoch [10/10], Step [440/495], Loss: 0.6768\n",
      "Epoch [10/10], Step [450/495], Loss: 1.5128\n",
      "Epoch [10/10], Step [460/495], Loss: 0.6383\n",
      "Epoch [10/10], Step [470/495], Loss: 1.5237\n",
      "Epoch [10/10], Step [480/495], Loss: 1.0866\n",
      "Epoch [10/10], Step [490/495], Loss: 3.3434\n",
      "Test Accuracy of the model on the test images: 57.37051792828685 %\n"
     ]
    }
   ],
   "source": [
    " # 导入所需库\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F \n",
    "\n",
    "# 定义超参数\n",
    "input_dim = 1 # 输入数据维度 \n",
    "num_classes = 2 # 分类数\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "\n",
    "# 定义CNN模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=5), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, num_classes) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out) \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# 实例化模型,判断GPU是否可用\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device) \n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练\n",
    "for epoch in range(epochs):\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device) # 数据到GPU\n",
    "        y = y.to(device) \n",
    "        optimizer.zero_grad()\n",
    "        #print(x.shape)\n",
    "        outputs = model(x)   \n",
    "        # 将y变成0D的tensor\n",
    "        y = y.squeeze(1)\n",
    "        outputs = F.log_softmax(outputs, dim=1)\n",
    "        # print(outputs.shape)\n",
    "        # print(y.shape)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 10 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, len(train_loader), loss.item()))\n",
    "# 测试\n",
    "model.eval()\n",
    "\n",
    "# 计算测试集准确率\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(x)\n",
    "        y = y.squeeze(1)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T14:50:19.059695700Z",
     "start_time": "2023-08-23T14:50:09.929435500Z"
    }
   },
   "id": "595046b354044fac"
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2953399504.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\shimmer\\AppData\\Local\\Temp\\ipykernel_4812\\2953399504.py\"\u001B[1;36m, line \u001B[1;32m1\u001B[0m\n\u001B[1;33m    |\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "|\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T14:43:12.532688700Z",
     "start_time": "2023-08-23T14:43:12.502591Z"
    }
   },
   "id": "8d7233823fa620ba"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "92b41fa7c58d96dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-23T14:36:06.946039700Z"
    }
   },
   "id": "aedc307d6d309d4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-23T14:36:06.948157900Z"
    }
   },
   "id": "212ae13ad8c92cda"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
