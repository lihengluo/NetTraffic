{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-27T08:55:54.604163700Z",
     "start_time": "2023-08-27T08:55:53.206810600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 读取数据， 第一行是列名\n",
    "labels = pd.read_csv('../data/labels.csv', header=0)\n",
    "featuress = pd.read_csv('../data/features.csv', header=0)\n",
    "# 将数据转换为numpy数组\n",
    "# 如果label为yes，转换为1，否则转换为0\n",
    "for i in range(len(labels)):\n",
    "    if labels['yes/no'][i] == 'yes':\n",
    "        labels['yes/no'][i] = 1\n",
    "    else:\n",
    "        labels['yes/no'][i] = 0\n",
    "labels = np.array(labels).astype(int)\n",
    "featuress = np.array(featuress).astype(float)\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(featuress, labels, test_size=0.3, random_state=0)\n",
    "\n",
    "# 将X_train 增加一个维度，值为1\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "# print(X_trian.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T08:55:58.622248800Z",
     "start_time": "2023-08-27T08:55:54.608246300Z"
    }
   },
   "id": "d396ee1a0475be3d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [10/495], Loss: 0.6932\n",
      "Epoch [1/100], Step [20/495], Loss: 0.6907\n",
      "Epoch [1/100], Step [30/495], Loss: 0.7021\n",
      "Epoch [1/100], Step [40/495], Loss: 0.6927\n",
      "Epoch [1/100], Step [50/495], Loss: 0.6974\n",
      "Epoch [1/100], Step [60/495], Loss: 0.6985\n",
      "Epoch [1/100], Step [70/495], Loss: 0.6915\n",
      "Epoch [1/100], Step [80/495], Loss: 0.6972\n",
      "Epoch [1/100], Step [90/495], Loss: 0.7004\n",
      "Epoch [1/100], Step [100/495], Loss: 0.6871\n",
      "Epoch [1/100], Step [110/495], Loss: 0.7028\n",
      "Epoch [1/100], Step [120/495], Loss: 0.6900\n",
      "Epoch [1/100], Step [130/495], Loss: 0.6846\n",
      "Epoch [1/100], Step [140/495], Loss: 0.6944\n",
      "Epoch [1/100], Step [150/495], Loss: 0.6927\n",
      "Epoch [1/100], Step [160/495], Loss: 0.6950\n",
      "Epoch [1/100], Step [170/495], Loss: 0.6924\n",
      "Epoch [1/100], Step [180/495], Loss: 0.6957\n",
      "Epoch [1/100], Step [190/495], Loss: 0.6918\n",
      "Epoch [1/100], Step [200/495], Loss: 0.6897\n",
      "Epoch [1/100], Step [210/495], Loss: 0.6991\n",
      "Epoch [1/100], Step [220/495], Loss: 0.6891\n",
      "Epoch [1/100], Step [230/495], Loss: 0.7026\n",
      "Epoch [1/100], Step [240/495], Loss: 0.6921\n",
      "Epoch [1/100], Step [250/495], Loss: 0.6940\n",
      "Epoch [1/100], Step [260/495], Loss: 0.6956\n",
      "Epoch [1/100], Step [270/495], Loss: 0.6987\n",
      "Epoch [1/100], Step [280/495], Loss: 0.6946\n",
      "Epoch [1/100], Step [290/495], Loss: 0.6835\n",
      "Epoch [1/100], Step [300/495], Loss: 0.6972\n",
      "Epoch [1/100], Step [310/495], Loss: 0.6924\n",
      "Epoch [1/100], Step [320/495], Loss: 0.7113\n",
      "Epoch [1/100], Step [330/495], Loss: 0.6995\n",
      "Epoch [1/100], Step [340/495], Loss: 0.6740\n",
      "Epoch [1/100], Step [350/495], Loss: 0.6973\n",
      "Epoch [1/100], Step [360/495], Loss: 0.6911\n",
      "Epoch [1/100], Step [370/495], Loss: 0.6974\n",
      "Epoch [1/100], Step [380/495], Loss: 0.6884\n",
      "Epoch [1/100], Step [390/495], Loss: 0.6835\n",
      "Epoch [1/100], Step [400/495], Loss: 0.6946\n",
      "Epoch [1/100], Step [410/495], Loss: 0.6916\n",
      "Epoch [1/100], Step [420/495], Loss: 0.6798\n",
      "Epoch [1/100], Step [430/495], Loss: 0.6924\n",
      "Epoch [1/100], Step [440/495], Loss: 0.6944\n",
      "Epoch [1/100], Step [450/495], Loss: 0.6986\n",
      "Epoch [1/100], Step [460/495], Loss: 0.7031\n",
      "Epoch [1/100], Step [470/495], Loss: 0.6806\n",
      "Epoch [1/100], Step [480/495], Loss: 0.6966\n",
      "Epoch [1/100], Step [490/495], Loss: 0.6909\n",
      "Epoch [2/100], Step [10/495], Loss: 0.6991\n",
      "Epoch [2/100], Step [20/495], Loss: 0.6990\n",
      "Epoch [2/100], Step [30/495], Loss: 0.6848\n",
      "Epoch [2/100], Step [40/495], Loss: 0.6884\n",
      "Epoch [2/100], Step [50/495], Loss: 0.6923\n",
      "Epoch [2/100], Step [60/495], Loss: 0.7016\n",
      "Epoch [2/100], Step [70/495], Loss: 0.6953\n",
      "Epoch [2/100], Step [80/495], Loss: 0.6960\n",
      "Epoch [2/100], Step [90/495], Loss: 0.6898\n",
      "Epoch [2/100], Step [100/495], Loss: 0.6912\n",
      "Epoch [2/100], Step [110/495], Loss: 0.6954\n",
      "Epoch [2/100], Step [120/495], Loss: 0.6936\n",
      "Epoch [2/100], Step [130/495], Loss: 0.6906\n",
      "Epoch [2/100], Step [140/495], Loss: 0.6877\n",
      "Epoch [2/100], Step [150/495], Loss: 0.6916\n",
      "Epoch [2/100], Step [160/495], Loss: 0.6908\n",
      "Epoch [2/100], Step [170/495], Loss: 0.6908\n",
      "Epoch [2/100], Step [180/495], Loss: 0.6876\n",
      "Epoch [2/100], Step [190/495], Loss: 0.7014\n",
      "Epoch [2/100], Step [200/495], Loss: 0.6947\n",
      "Epoch [2/100], Step [210/495], Loss: 0.6824\n",
      "Epoch [2/100], Step [220/495], Loss: 0.6874\n",
      "Epoch [2/100], Step [230/495], Loss: 0.6860\n",
      "Epoch [2/100], Step [240/495], Loss: 0.6929\n",
      "Epoch [2/100], Step [250/495], Loss: 0.6910\n",
      "Epoch [2/100], Step [260/495], Loss: 0.7028\n",
      "Epoch [2/100], Step [270/495], Loss: 0.6937\n",
      "Epoch [2/100], Step [280/495], Loss: 0.6896\n",
      "Epoch [2/100], Step [290/495], Loss: 0.6948\n",
      "Epoch [2/100], Step [300/495], Loss: 0.6800\n",
      "Epoch [2/100], Step [310/495], Loss: 0.6947\n",
      "Epoch [2/100], Step [320/495], Loss: 0.6948\n",
      "Epoch [2/100], Step [330/495], Loss: 0.6912\n",
      "Epoch [2/100], Step [340/495], Loss: 0.6909\n",
      "Epoch [2/100], Step [350/495], Loss: 0.6910\n",
      "Epoch [2/100], Step [360/495], Loss: 0.6911\n",
      "Epoch [2/100], Step [370/495], Loss: 0.6883\n",
      "Epoch [2/100], Step [380/495], Loss: 0.6813\n",
      "Epoch [2/100], Step [390/495], Loss: 0.7016\n",
      "Epoch [2/100], Step [400/495], Loss: 0.7180\n",
      "Epoch [2/100], Step [410/495], Loss: 0.6975\n",
      "Epoch [2/100], Step [420/495], Loss: 0.6907\n",
      "Epoch [2/100], Step [430/495], Loss: 0.6953\n",
      "Epoch [2/100], Step [440/495], Loss: 0.7009\n",
      "Epoch [2/100], Step [450/495], Loss: 0.6854\n",
      "Epoch [2/100], Step [460/495], Loss: 0.6971\n",
      "Epoch [2/100], Step [470/495], Loss: 0.7001\n",
      "Epoch [2/100], Step [480/495], Loss: 0.7006\n",
      "Epoch [2/100], Step [490/495], Loss: 0.6925\n",
      "Epoch [3/100], Step [10/495], Loss: 0.6918\n",
      "Epoch [3/100], Step [20/495], Loss: 0.6873\n",
      "Epoch [3/100], Step [30/495], Loss: 0.6963\n",
      "Epoch [3/100], Step [40/495], Loss: 0.6981\n",
      "Epoch [3/100], Step [50/495], Loss: 0.7008\n",
      "Epoch [3/100], Step [60/495], Loss: 0.6923\n",
      "Epoch [3/100], Step [70/495], Loss: 0.6919\n",
      "Epoch [3/100], Step [80/495], Loss: 0.6953\n",
      "Epoch [3/100], Step [90/495], Loss: 0.6902\n",
      "Epoch [3/100], Step [100/495], Loss: 0.6873\n",
      "Epoch [3/100], Step [110/495], Loss: 0.6882\n",
      "Epoch [3/100], Step [120/495], Loss: 0.6958\n",
      "Epoch [3/100], Step [130/495], Loss: 0.6886\n",
      "Epoch [3/100], Step [140/495], Loss: 0.6844\n",
      "Epoch [3/100], Step [150/495], Loss: 0.6926\n",
      "Epoch [3/100], Step [160/495], Loss: 0.6930\n",
      "Epoch [3/100], Step [170/495], Loss: 0.6867\n",
      "Epoch [3/100], Step [180/495], Loss: 0.6868\n",
      "Epoch [3/100], Step [190/495], Loss: 0.6844\n",
      "Epoch [3/100], Step [200/495], Loss: 0.7133\n",
      "Epoch [3/100], Step [210/495], Loss: 0.6892\n",
      "Epoch [3/100], Step [220/495], Loss: 0.6870\n",
      "Epoch [3/100], Step [230/495], Loss: 0.6979\n",
      "Epoch [3/100], Step [240/495], Loss: 0.7036\n",
      "Epoch [3/100], Step [250/495], Loss: 0.6889\n",
      "Epoch [3/100], Step [260/495], Loss: 0.6922\n",
      "Epoch [3/100], Step [270/495], Loss: 0.6885\n",
      "Epoch [3/100], Step [280/495], Loss: 0.6885\n",
      "Epoch [3/100], Step [290/495], Loss: 0.6990\n",
      "Epoch [3/100], Step [300/495], Loss: 0.6854\n",
      "Epoch [3/100], Step [310/495], Loss: 0.6845\n",
      "Epoch [3/100], Step [320/495], Loss: 0.6856\n",
      "Epoch [3/100], Step [330/495], Loss: 0.6793\n",
      "Epoch [3/100], Step [340/495], Loss: 0.6865\n",
      "Epoch [3/100], Step [350/495], Loss: 0.6921\n",
      "Epoch [3/100], Step [360/495], Loss: 0.6878\n",
      "Epoch [3/100], Step [370/495], Loss: 0.6987\n",
      "Epoch [3/100], Step [380/495], Loss: 0.6867\n",
      "Epoch [3/100], Step [390/495], Loss: 0.6939\n",
      "Epoch [3/100], Step [400/495], Loss: 0.6971\n",
      "Epoch [3/100], Step [410/495], Loss: 0.6912\n",
      "Epoch [3/100], Step [420/495], Loss: 0.6921\n",
      "Epoch [3/100], Step [430/495], Loss: 0.6840\n",
      "Epoch [3/100], Step [440/495], Loss: 0.6942\n",
      "Epoch [3/100], Step [450/495], Loss: 0.6921\n",
      "Epoch [3/100], Step [460/495], Loss: 0.6841\n",
      "Epoch [3/100], Step [470/495], Loss: 0.6922\n",
      "Epoch [3/100], Step [480/495], Loss: 0.6834\n",
      "Epoch [3/100], Step [490/495], Loss: 0.7003\n",
      "Epoch [4/100], Step [10/495], Loss: 0.6926\n",
      "Epoch [4/100], Step [20/495], Loss: 0.6904\n",
      "Epoch [4/100], Step [30/495], Loss: 0.6986\n",
      "Epoch [4/100], Step [40/495], Loss: 0.7026\n",
      "Epoch [4/100], Step [50/495], Loss: 0.6918\n",
      "Epoch [4/100], Step [60/495], Loss: 0.6884\n",
      "Epoch [4/100], Step [70/495], Loss: 0.6911\n",
      "Epoch [4/100], Step [80/495], Loss: 0.6907\n",
      "Epoch [4/100], Step [90/495], Loss: 0.6879\n",
      "Epoch [4/100], Step [100/495], Loss: 0.7077\n",
      "Epoch [4/100], Step [110/495], Loss: 0.6934\n",
      "Epoch [4/100], Step [120/495], Loss: 0.7028\n",
      "Epoch [4/100], Step [130/495], Loss: 0.6922\n",
      "Epoch [4/100], Step [140/495], Loss: 0.6879\n",
      "Epoch [4/100], Step [150/495], Loss: 0.6896\n",
      "Epoch [4/100], Step [160/495], Loss: 0.6845\n",
      "Epoch [4/100], Step [170/495], Loss: 0.6931\n",
      "Epoch [4/100], Step [180/495], Loss: 0.6909\n",
      "Epoch [4/100], Step [190/495], Loss: 0.6971\n",
      "Epoch [4/100], Step [200/495], Loss: 0.6975\n",
      "Epoch [4/100], Step [210/495], Loss: 0.6899\n",
      "Epoch [4/100], Step [220/495], Loss: 0.6925\n",
      "Epoch [4/100], Step [230/495], Loss: 0.6856\n",
      "Epoch [4/100], Step [240/495], Loss: 0.6886\n",
      "Epoch [4/100], Step [250/495], Loss: 0.6871\n",
      "Epoch [4/100], Step [260/495], Loss: 0.6856\n",
      "Epoch [4/100], Step [270/495], Loss: 0.6911\n",
      "Epoch [4/100], Step [280/495], Loss: 0.6931\n",
      "Epoch [4/100], Step [290/495], Loss: 0.6881\n",
      "Epoch [4/100], Step [300/495], Loss: 0.6817\n",
      "Epoch [4/100], Step [310/495], Loss: 0.6893\n",
      "Epoch [4/100], Step [320/495], Loss: 0.6861\n",
      "Epoch [4/100], Step [330/495], Loss: 0.6929\n",
      "Epoch [4/100], Step [340/495], Loss: 0.6867\n",
      "Epoch [4/100], Step [350/495], Loss: 0.6893\n",
      "Epoch [4/100], Step [360/495], Loss: 0.6855\n",
      "Epoch [4/100], Step [370/495], Loss: 0.6802\n",
      "Epoch [4/100], Step [380/495], Loss: 0.6966\n",
      "Epoch [4/100], Step [390/495], Loss: 0.6793\n",
      "Epoch [4/100], Step [400/495], Loss: 0.6903\n",
      "Epoch [4/100], Step [410/495], Loss: 0.6773\n",
      "Epoch [4/100], Step [420/495], Loss: 0.6923\n",
      "Epoch [4/100], Step [430/495], Loss: 0.7038\n",
      "Epoch [4/100], Step [440/495], Loss: 0.6771\n",
      "Epoch [4/100], Step [450/495], Loss: 0.6849\n",
      "Epoch [4/100], Step [460/495], Loss: 0.6865\n",
      "Epoch [4/100], Step [470/495], Loss: 0.6883\n",
      "Epoch [4/100], Step [480/495], Loss: 0.6838\n",
      "Epoch [4/100], Step [490/495], Loss: 0.6761\n",
      "Epoch [5/100], Step [10/495], Loss: 0.6951\n",
      "Epoch [5/100], Step [20/495], Loss: 0.6916\n",
      "Epoch [5/100], Step [30/495], Loss: 0.6874\n",
      "Epoch [5/100], Step [40/495], Loss: 0.6889\n",
      "Epoch [5/100], Step [50/495], Loss: 0.6904\n",
      "Epoch [5/100], Step [60/495], Loss: 0.6888\n",
      "Epoch [5/100], Step [70/495], Loss: 0.6963\n",
      "Epoch [5/100], Step [80/495], Loss: 0.6874\n",
      "Epoch [5/100], Step [90/495], Loss: 0.7020\n",
      "Epoch [5/100], Step [100/495], Loss: 0.6893\n",
      "Epoch [5/100], Step [110/495], Loss: 0.6921\n",
      "Epoch [5/100], Step [120/495], Loss: 0.6941\n",
      "Epoch [5/100], Step [130/495], Loss: 0.6896\n",
      "Epoch [5/100], Step [140/495], Loss: 0.6818\n",
      "Epoch [5/100], Step [150/495], Loss: 0.6831\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_23620\\478221040.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     54\u001B[0m        \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m        \u001B[1;31m#print(x.shape)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m        \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m        \u001B[1;31m# 将y变成0D的tensor\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m        \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1111\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_23620\\478221040.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     31\u001B[0m        \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m        \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 33\u001B[1;33m        \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfc2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     34\u001B[0m        \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDropout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     35\u001B[0m        \u001B[1;32mreturn\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1111\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    139\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    140\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 141\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    142\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1111\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    101\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    102\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 103\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    104\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    105\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    " # 导入所需库\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F \n",
    "\n",
    "# 定义超参数\n",
    "input_dim = 1 # 输入数据维度 \n",
    "num_classes = 2 # 分类数\n",
    "batch_size = 64 # 批处理数\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "# 定义CNN模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "            \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc2(out)\n",
    "        # 使用dropout技术\n",
    "        out = nn.Dropout(p=0.5)(out)\n",
    "        return out\n",
    "\n",
    "# 实例化模型,判断GPU是否可用\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device) \n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# 定义损失函数和优化器 损失函数为交叉熵\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练\n",
    "for epoch in range(epochs):\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device) # 数据到GPU\n",
    "        y = y.to(device) \n",
    "        #使用随机梯度下降作为优化器，学习率衰减\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate*0.96)\n",
    "        optimizer.zero_grad()\n",
    "        #print(x.shape)\n",
    "        outputs = model(x)   \n",
    "        # 将y变成0D的tensor\n",
    "        y = y.squeeze(1)\n",
    "        outputs = F.log_softmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 10 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, len(train_loader), loss.item()))\n",
    "# 测试\n",
    "model.eval()\n",
    "\n",
    "# 计算测试集准确率\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(x)\n",
    "        y = y.squeeze(1)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T09:09:55.763351300Z",
     "start_time": "2023-08-27T09:09:52.348896600Z"
    }
   },
   "id": "595046b354044fac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-27T08:56:06.269843100Z"
    }
   },
   "id": "8d7233823fa620ba"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "92b41fa7c58d96dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-27T08:56:06.269843100Z"
    }
   },
   "id": "aedc307d6d309d4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-27T08:56:06.271842600Z"
    }
   },
   "id": "212ae13ad8c92cda"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
